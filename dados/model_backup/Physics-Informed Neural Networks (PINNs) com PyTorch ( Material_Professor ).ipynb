{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "935f42eb",
   "metadata": {},
   "source": [
    "Sumário: \n",
    "\n",
    "1. Physics-Informed Neural Networks (PINNs) com PyTorch\n",
    "\n",
    "1. **Introdução às PINNs**\n",
    "    - O que são PINNs?\n",
    "    - Por que combinar física com redes neurais?\n",
    "    - Aplicações em finanças e outras áreas.\n",
    "\n",
    "2. **Fundamentos Matemáticos das PINNs**\n",
    "    - Formulação de equações diferenciais parciais (PDEs) em PINNs.\n",
    "    - Função de perda combinando dados e física.\n",
    "    - Exemplo simples: Equação do Calor ou Black-Scholes.\n",
    "\n",
    "3. **Implementação Básica com PyTorch**\n",
    "    - Arquitetura da rede neural.\n",
    "    - Definição da função de perda (dados + PDE).\n",
    "    - Treinamento e desafios numéricos.\n",
    "\n",
    "4. **Aplicação em Finanças: Modelo Black-Scholes**\n",
    "    - Revisão da equação Black-Scholes.\n",
    "    - Adaptação para uma PINN.\n",
    "    - Comparação com soluções analíticas/númericas tradicionais.\n",
    "\n",
    "5. **Tópicos Avançados e Otimizações**\n",
    "    - Tratamento de condições iniciais/de contorno.\n",
    "    - Balanceamento de termos na função de perda.\n",
    "    - Aceleração com GPUs e técnicas de treinamento.\n",
    "\n",
    "6. **Estudo de Caso Prático**\n",
    "    - Implementação completa de uma PINN para opções europeias.\n",
    "    - Visualização de resultados.\n",
    "\n",
    "7. **Limitações e Extensões**\n",
    "    - Quando PINNs falham?\n",
    "    - Alternativas híbridas (ex: PINNs + SDEs)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb0e2d0",
   "metadata": {},
   "source": [
    "## 1. Physics-Informed Neural Networks (PINNs) com PyTorch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f9be1e",
   "metadata": {},
   "source": [
    "### 1.1 **Introdução às PINNs**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa92f78f",
   "metadata": {},
   "source": [
    "1.1 **Introdução às PINNs**\n",
    "- O que são PINNs?\n",
    "- Por que combinar física com redes neurais?\n",
    "- Aplicações em finanças e outras áreas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f385a3d5",
   "metadata": {},
   "source": [
    "O que são PINNs?\n",
    "PINNs são redes neurais treinadas para resolver equações diferenciais (PDEs/ODEs) incorporando diretamente as leis físicas (ou financeiras) na função de perda. Em vez de depender apenas de dados, elas usam a estrutura conhecida da equação subjacente (ex: Black-Scholes) como regularização.\n",
    "\n",
    "Por que combinar física com redes neurais?\n",
    "- Dados escassos: Em problemas financeiros ou físicos, dados reais podem ser limitados ou ruidosos. A física atua como um \"guia\" para generalização.\n",
    "- Interpretabilidade: A solução respeita leis conhecidas, mesmo em regiões sem dados.\n",
    "- Flexibilidade: PINNs lidam com problemas não-lineares e geometrias complexas sem malhas numéricas.\n",
    "\n",
    "Aplicações em finanças:\n",
    "- Precificação de opções (Black-Scholes).\n",
    "- Calibração de modelos estocásticos.\n",
    "- Simulação de cenários de mercado com restrições teóricas.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "197434ce",
   "metadata": {},
   "source": [
    "### 2. **Fundamentos Matemáticos das PINNs**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f08a066",
   "metadata": {},
   "source": [
    "**Fundamentos Matemáticos das PINNs**\n",
    "    - Formulação de equações diferenciais parciais (PDEs) em PINNs.\n",
    "    - Função de perda combinando dados e física.\n",
    "    - Exemplo simples: Equação do Calor ou Black-Scholes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "954aee46",
   "metadata": {},
   "source": [
    "As PINNs resolvem problemas baseados em equações diferenciais (PDEs/ODEs) combinando duas fontes de informação:  \n",
    "1. **Dados observados** (ex: preços de opções no mercado).  \n",
    "2. **Leis físicas/financeiras** (ex: equação Black-Scholes).  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba0ef56",
   "metadata": {},
   "source": [
    "#### **2.1 Formulação Geral de uma PINN**  \n",
    "Considere uma PDE genérica:  \n",
    "$$\n",
    "\\mathcal{F}[u(t, x)] = 0, \\quad t \\in [0, T], x \\in \\Omega  \n",
    "$$  \n",
    "com:  \n",
    "- **Condição inicial (IC):** \\( u(0, x) = u_0(x) \\).  \n",
    "- **Condições de contorno (BC):** \\( u(t, x) = g(t, x) \\) em \\( \\partial\\Omega \\).  \n",
    "\n",
    "**Exemplo (Equação Black-Scholes):**  \n",
    "$$\n",
    "\\frac{\\partial V}{\\partial t} + rS\\frac{\\partial V}{\\partial S} + \\frac{1}{2}\\sigma^2 S^2 \\frac{\\partial^2 V}{\\partial S^2} - rV = 0  \n",
    "$$  \n",
    "Onde \\( V(t, S) \\) é o preço da opção, \\( S \\) é o ativo subjacente, \\( r \\) é a taxa livre de risco, e \\( \\sigma \\) é a volatilidade.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "883fddf0",
   "metadata": {},
   "source": [
    "A ordem lógica do cógido se da pela seguinte forma:\n",
    "\n",
    "Código Completo da PINN para Black-Scholes\n",
    "1. Importações e Configurações Iniciais\n",
    "2. Definição da Arquitetura da PINN\n",
    "3. Funções de Amostragem de Dados\n",
    "4. Funções de Perda\n",
    "5. Loop de Treinamento\n",
    "\n",
    "\n",
    "Ordem Lógica de Execução\n",
    "- Configuração: Define parâmetros do modelo e hiperparâmetros da rede.\n",
    "- \n",
    "- Arquitetura: Cria a PINN com camadas lineares e ativação Tanh.\n",
    "- \n",
    "- Amostragem: Gera pontos para o domínio, condição inicial e contorno.\n",
    "- \n",
    "- Perdas: Calcula os termos da PDE, condição inicial e contorno.\n",
    "- \n",
    "- Treinamento: Minimiza a perda total via backpropagation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7afcbd12",
   "metadata": {},
   "source": [
    "# A Equação de Black-Scholes\n",
    "\n",
    "A equação de Black-Scholes é uma PDE (Equação Diferencial Parcial) que descreve a evolução do preço de uma opção europeia sob certas premissas, como:\n",
    "- Mercado eficiente;\n",
    "- Sem pagamento de dividendos;\n",
    "- Volatilidade constante.\n",
    "\n",
    "Para uma **opção de compra (call)**, sua fórmula analítica é:\n",
    "\n",
    "$$\n",
    "C(S_t, t) = S_t N(d_1) - K e^{-r(T-t)} N(d_2)\n",
    "$$\n",
    "\n",
    "Onde:\n",
    "- \\( C(S_t, t) \\) é o preço da opção no tempo \\( t \\), dado o preço do ativo \\( S_t \\).\n",
    "- \\( N(\\cdot) \\) é a função cumulativa da distribuição normal padrão.\n",
    "- \\( d_1 \\) e \\( d_2 \\) são termos que dependem de \\( S_t \\), \\( K \\), \\( r \\), \\( \\sigma \\) e \\( T - t \\).\n",
    "\n",
    "### Forma Diferencial (PDE) da Equação de Black-Scholes\n",
    "\n",
    "A forma diferencial da equação, que é a base para a PINN, é:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial V}{\\partial t} + r S \\frac{\\partial V}{\\partial S} + \\frac{1}{2} \\sigma^2 S^2 \\frac{\\partial^2 V}{\\partial S^2} - rV = 0\n",
    "$$\n",
    "\n",
    "Essa equação é a base para diversas abordagens numéricas e de aprendizado de máquina, como redes neurais físicas (*Physics-Informed Neural Networks* - PINNs), que buscam resolver a PDE de forma eficiente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca77e79c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Código para Treinamento de uma Rede Neural Profunda (PINN) para o Modelo Black-Scholes\n",
    "#1. Importações e Configurações Iniciais\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "# Parâmetros do modelo Black-Scholes\n",
    "r = 0.05      # Taxa livre de risco\n",
    "sigma = 0.2   # Volatilidade\n",
    "K = 100.0     # Preço de exercício (strike)\n",
    "T = 1.0       # Tempo até a expiração (em anos)\n",
    "S_min = 0     # Preço mínimo do ativo\n",
    "S_max = 200   # Preço máximo do ativo\n",
    "\n",
    "# Hiperparâmetros da PINN\n",
    "layers = [2, 50, 50, 1]  # Arquitetura da rede: [input_dim, hidden_dim, ..., output_dim]\n",
    "lambda_pde = 1.0         # Peso da perda da PDE\n",
    "lambda_ic = 1.0          # Peso da condição inicial\n",
    "lambda_bc = 1.0          # Peso das condições de contorno\n",
    "batch_size = 100         # Número de pontos por batch\n",
    "epochs = 5000            # Número de épocas\n",
    "\n",
    "#2. Definição da Arquitetura da PINN\n",
    "class PINN(nn.Module):\n",
    "    def __init__(self, layers):\n",
    "        super().__init__()\n",
    "        self.linear_layers = nn.ModuleList(\n",
    "            [nn.Linear(layers[i], layers[i+1]) for i in range(len(layers)-1)]\n",
    "        )\n",
    "        self.activation = nn.Tanh()  # Ativação suave para gradientes estáveis\n",
    "\n",
    "    def forward(self, t, S):\n",
    "        inputs = torch.cat([t, S], dim=1)  # Concatena tempo (t) e preço (S)\n",
    "        for layer in self.linear_layers[:-1]:\n",
    "            inputs = self.activation(layer(inputs))\n",
    "        output = self.linear_layers[-1](inputs)  # Sem ativação na última camada\n",
    "        return output\n",
    "    \n",
    "#3. Funções de Amostragem de Dados\n",
    "def sample_domain(batch_size):\n",
    "    \"\"\"Gera pontos aleatórios no domínio [0,T] x [S_min, S_max].\"\"\"\n",
    "    t = torch.rand(batch_size, 1) * T\n",
    "    S = torch.rand(batch_size, 1) * (S_max - S_min) + S_min\n",
    "    return t, S\n",
    "\n",
    "def sample_ic(batch_size):\n",
    "    \"\"\"Gera pontos para a condição inicial (payoff em t=T).\"\"\"\n",
    "    S_ic = torch.rand(batch_size, 1) * (S_max - S_min) + S_min\n",
    "    return S_ic\n",
    "\n",
    "def sample_bc_times(batch_size):\n",
    "    \"\"\"Gera tempos para as condições de contorno.\"\"\"\n",
    "    return torch.rand(batch_size, 1) * T\n",
    "\n",
    "#4. Funções de Perda\n",
    "def loss_pde(t, S, V_hat):\n",
    "    \"\"\"Calcula o residual da PDE Black-Scholes.\"\"\"\n",
    "    t.requires_grad_(True)\n",
    "    S.requires_grad_(True)\n",
    "    \n",
    "    # Derivadas parciais via autograd\n",
    "    dV_dt = torch.autograd.grad(V_hat.sum(), t, create_graph=True)[0]\n",
    "    dV_dS = torch.autograd.grad(V_hat.sum(), S, create_graph=True)[0]\n",
    "    d2V_dS2 = torch.autograd.grad(dV_dS.sum(), S, create_graph=True)[0]\n",
    "    \n",
    "    # Residual da PDE\n",
    "    residual = dV_dt + r * S * dV_dS + 0.5 * (sigma ** 2) * (S ** 2) * d2V_dS2 - r * V_hat\n",
    "    return (residual ** 2).mean()\n",
    "\n",
    "def loss_ic(S_ic):\n",
    "    \"\"\"Perda da condição inicial (payoff final).\"\"\"\n",
    "    t_ic = torch.ones_like(S_ic) * T  # Tempo final (t=T)\n",
    "    V_true_ic = torch.max(S_ic - K, torch.zeros_like(S_ic))  # Payoff: max(S-K, 0)\n",
    "    V_pred_ic = pinn(t_ic, S_ic)\n",
    "    return ((V_pred_ic - V_true_ic) ** 2).mean()\n",
    "\n",
    "def loss_bc(t_bc):\n",
    "    \"\"\"Perda das condições de contorno.\"\"\"\n",
    "    # BC1: V(t, S=0) = 0\n",
    "    S_bc_low = torch.zeros_like(t_bc)\n",
    "    V_bc_low = pinn(t_bc, S_bc_low)\n",
    "    loss_bc1 = (V_bc_low ** 2).mean()\n",
    "    \n",
    "    # BC2: V(t, S -> ∞) ≈ S - K*e^{-r(T-t)}\n",
    "    S_bc_high = torch.ones_like(t_bc) * S_max\n",
    "    V_bc_high = pinn(t_bc, S_bc_high)\n",
    "    V_true_bc_high = S_bc_high - K * torch.exp(-r * (T - t_bc))\n",
    "    loss_bc2 = ((V_bc_high - V_true_bc_high) ** 2).mean()\n",
    "    \n",
    "    return loss_bc1 + loss_bc2\n",
    "\n",
    "#5. Loop de Treinamento\n",
    "# Inicializa a rede e o otimizador\n",
    "pinn = PINN(layers)\n",
    "optimizer = torch.optim.Adam(pinn.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Amostra pontos do domínio e calcula as perdas\n",
    "    t_pde, S_pde = sample_domain(batch_size)\n",
    "    V_hat = pinn(t_pde, S_pde)\n",
    "    \n",
    "    l_pde = loss_pde(t_pde, S_pde, V_hat)\n",
    "    l_ic = loss_ic(sample_ic(batch_size))\n",
    "    l_bc = loss_bc(sample_bc_times(batch_size))\n",
    "    \n",
    "    # Perda total ponderada\n",
    "    loss = lambda_pde * l_pde + lambda_ic * l_ic + lambda_bc * l_bc\n",
    "    \n",
    "    # Backpropagation\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if epoch % 1000 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss.item():.4f}, PDE Loss: {l_pde.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0a31dbd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.10.1-cp313-cp313-win_amd64.whl.metadata (11 kB)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib)\n",
      "  Downloading contourpy-1.3.1-cp313-cp313-win_amd64.whl.metadata (5.4 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib)\n",
      "  Downloading cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib)\n",
      "  Downloading fonttools-4.57.0-cp313-cp313-win_amd64.whl.metadata (104 kB)\n",
      "Collecting kiwisolver>=1.3.1 (from matplotlib)\n",
      "  Downloading kiwisolver-1.4.8-cp313-cp313-win_amd64.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: numpy>=1.23 in d:\\programas\\python e visual basics\\anaconda.python\\envs\\ambiente\\lib\\site-packages (from matplotlib) (2.2.4)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\programas\\python e visual basics\\anaconda.python\\envs\\ambiente\\lib\\site-packages (from matplotlib) (24.2)\n",
      "Collecting pillow>=8 (from matplotlib)\n",
      "  Downloading pillow-11.2.1-cp313-cp313-win_amd64.whl.metadata (9.1 kB)\n",
      "Collecting pyparsing>=2.3.1 (from matplotlib)\n",
      "  Downloading pyparsing-3.2.3-py3-none-any.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in d:\\programas\\python e visual basics\\anaconda.python\\envs\\ambiente\\lib\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in d:\\programas\\python e visual basics\\anaconda.python\\envs\\ambiente\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Downloading matplotlib-3.10.1-cp313-cp313-win_amd64.whl (8.1 MB)\n",
      "   ---------------------------------------- 0.0/8.1 MB ? eta -:--:--\n",
      "   ---------- ----------------------------- 2.1/8.1 MB 11.7 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 4.5/8.1 MB 11.7 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 7.1/8.1 MB 11.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 8.1/8.1 MB 11.5 MB/s eta 0:00:00\n",
      "Downloading contourpy-1.3.1-cp313-cp313-win_amd64.whl (220 kB)\n",
      "Downloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Downloading fonttools-4.57.0-cp313-cp313-win_amd64.whl (2.2 MB)\n",
      "   ---------------------------------------- 0.0/2.2 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.2/2.2 MB 11.2 MB/s eta 0:00:00\n",
      "Downloading kiwisolver-1.4.8-cp313-cp313-win_amd64.whl (71 kB)\n",
      "Downloading pillow-11.2.1-cp313-cp313-win_amd64.whl (2.7 MB)\n",
      "   ---------------------------------------- 0.0/2.7 MB ? eta -:--:--\n",
      "   ----------------------------------- ---- 2.4/2.7 MB 11.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.7/2.7 MB 11.1 MB/s eta 0:00:00\n",
      "Downloading pyparsing-3.2.3-py3-none-any.whl (111 kB)\n",
      "Installing collected packages: pyparsing, pillow, kiwisolver, fonttools, cycler, contourpy, matplotlib\n",
      "Successfully installed contourpy-1.3.1 cycler-0.12.1 fonttools-4.57.0 kiwisolver-1.4.8 matplotlib-3.10.1 pillow-11.2.1 pyparsing-3.2.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d14baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af33db73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Época     0 | Perda Total: 1.2887e+04 | Perda PDE: 1.0179e-02 | Perda IC: 2.3319e+03 | Perda BC: 1.0555e+04\n",
      "Época  1000 | Perda Total: 3.0909e+03 | Perda PDE: 9.0591e+00 | Perda IC: 2.8809e+02 | Perda BC: 2.7937e+03\n",
      "Época  2000 | Perda Total: 5.7378e+02 | Perda PDE: 1.9597e+01 | Perda IC: 2.8109e+01 | Perda BC: 5.2608e+02\n",
      "Época  3000 | Perda Total: 9.5734e+01 | Perda PDE: 8.6751e+00 | Perda IC: 8.4406e+00 | Perda BC: 7.8618e+01\n",
      "Época  4000 | Perda Total: 1.6335e+01 | Perda PDE: 9.0750e-01 | Perda IC: 1.0315e+00 | Perda BC: 1.4396e+01\n"
     ]
    }
   ],
   "source": [
    "#Código para Treinamento de uma Rede Neural Profunda (PINN) para o Modelo Black-Scholes\n",
    "\n",
    "# ======================\n",
    "# 1. Configuração\n",
    "# ======================\n",
    "r = 0.05       # Taxa livre de risco\n",
    "sigma = 0.2    # Volatilidade\n",
    "K = 100.0      # Preço de exercício\n",
    "T = 1.0        # Tempo até a maturidade (anos)\n",
    "S_min = 0      # Preço mínimo do ativo\n",
    "S_max = 200    # Preço máximo do ativo\n",
    "\n",
    "# Hiperparâmetros da PINN\n",
    "layers = [2, 50, 50, 1]  # Estrutura da rede [entrada, ocultas..., saída]\n",
    "lambda_pde = 1.0         # Peso da perda da PDE\n",
    "lambda_ic = 1.0          # Peso da perda da condição inicial\n",
    "lambda_bc = 1.0          # Peso da perda da condição de contorno\n",
    "batch_size = 100         # Pontos por lote\n",
    "epochs = 5000            # Número de épocas de treino\n",
    "learning_rate = 0.001    # Taxa de aprendizado do otimizador\n",
    "\n",
    "# ======================\n",
    "# 2. Arquitetura da PINN\n",
    "# ======================\n",
    "class BlackScholesPINN(nn.Module):\n",
    "    def __init__(self, layers):\n",
    "        super().__init__()\n",
    "        self.linear_layers = nn.ModuleList(\n",
    "            [nn.Linear(layers[i], layers[i+1]) for i in range(len(layers)-1)]\n",
    "        )\n",
    "        self.activation = nn.Tanh()  # Função de ativação suave para estabilidade\n",
    "\n",
    "    def forward(self, t, S):\n",
    "        t_norm = t.view(-1, 1) / T\n",
    "        S_norm = S.view(-1, 1) / K\n",
    "        inputs = torch.cat([t_norm, S_norm], dim=1)\n",
    "\n",
    "        for layer in self.linear_layers[:-1]:\n",
    "            inputs = self.activation(layer(inputs))\n",
    "        return self.linear_layers[-1](inputs)\n",
    "\n",
    "# ======================\n",
    "# 3. Amostragem de Dados\n",
    "# ======================\n",
    "def sample_domain(batch_size):\n",
    "    t = torch.rand(batch_size, 1) * T\n",
    "    S = torch.rand(batch_size, 1) * (S_max - S_min) + S_min\n",
    "    t.requires_grad = True\n",
    "    S.requires_grad = True\n",
    "    return t, S\n",
    "\n",
    "def sample_ic(batch_size):\n",
    "    S_ic = torch.rand(batch_size, 1) * (S_max - S_min) + S_min\n",
    "    return S_ic\n",
    "\n",
    "def sample_bc(batch_size):\n",
    "    t_bc = torch.rand(batch_size, 1) * T\n",
    "    return t_bc\n",
    "\n",
    "# ======================\n",
    "# 4. Funções de Perda\n",
    "# ======================\n",
    "def compute_derivatives(t, S, V_hat):\n",
    "    dV_dt = torch.autograd.grad(V_hat.sum(), t, create_graph=True)[0]\n",
    "    dV_dS = torch.autograd.grad(V_hat.sum(), S, create_graph=True)[0]\n",
    "    d2V_dS2 = torch.autograd.grad(dV_dS.sum(), S, create_graph=True)[0]\n",
    "    return dV_dt, dV_dS, d2V_dS2\n",
    "\n",
    "def pde_loss(t, S, V_hat):\n",
    "    dV_dt, dV_dS, d2V_dS2 = compute_derivatives(t, S, V_hat)\n",
    "    residual = dV_dt + r*S*dV_dS + 0.5*(sigma**2)*(S**2)*d2V_dS2 - r*V_hat\n",
    "    return (residual**2).mean()\n",
    "\n",
    "def ic_loss(pinn, S_ic):\n",
    "    t_ic = torch.ones_like(S_ic) * T\n",
    "    V_true = torch.max(S_ic - K, torch.zeros_like(S_ic))\n",
    "    V_pred = pinn(t_ic, S_ic)\n",
    "    return ((V_pred - V_true)**2).mean()\n",
    "\n",
    "def bc_loss(pinn, t_bc):\n",
    "    S_bc1 = torch.zeros_like(t_bc)\n",
    "    V_bc1 = pinn(t_bc, S_bc1)\n",
    "    loss1 = (V_bc1**2).mean()\n",
    "\n",
    "    S_bc2 = torch.ones_like(t_bc) * S_max\n",
    "    V_true_bc2 = S_bc2 - K * torch.exp(-r * (T - t_bc))\n",
    "    V_bc2 = pinn(t_bc, S_bc2)\n",
    "    loss2 = ((V_bc2 - V_true_bc2)**2).mean()\n",
    "\n",
    "    return loss1 + loss2\n",
    "\n",
    "# ======================\n",
    "# 5. Loop de Treinamento\n",
    "# ======================\n",
    "pinn = BlackScholesPINN(layers)\n",
    "optimizer = torch.optim.Adam(pinn.parameters(), lr=learning_rate)\n",
    "\n",
    "loss_history = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    t_pde, S_pde = sample_domain(batch_size)\n",
    "    V_hat = pinn(t_pde, S_pde)\n",
    "\n",
    "    l_pde = pde_loss(t_pde, S_pde, V_hat)\n",
    "    l_ic = ic_loss(pinn, sample_ic(batch_size))\n",
    "    l_bc = bc_loss(pinn, sample_bc(batch_size))\n",
    "\n",
    "    loss = lambda_pde*l_pde + lambda_ic*l_ic + lambda_bc*l_bc\n",
    "    loss_history.append(loss.item())\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 1000 == 0:\n",
    "        print(f\"Época {epoch:5d} | Perda Total: {loss.item():.4e} | \"\n",
    "              f\"Perda PDE: {l_pde.item():.4e} | Perda IC: {l_ic.item():.4e} | \"\n",
    "              f\"Perda BC: {l_bc.item():.4e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc8ad46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Etapa concluída: Configuração inicial\n",
      "✅ Etapa concluída: Definição da arquitetura da rede\n",
      "✅ Etapa concluída: Amostragem de dados\n",
      "✅ Etapa concluída: Definição das funções de perda\n",
      "✅ Etapa concluída: Inicialização da rede e otimizador\n",
      "Época     0 | Perda Total: 1.2130e+04 | Perda PDE: 3.0908e-02 | Perda IC: 1.5536e+03 | Perda BC: 1.0576e+04\n",
      "Época  1000 | Perda Total: 3.0298e+03 | Perda PDE: 6.2679e+00 | Perda IC: 2.3360e+02 | Perda BC: 2.7899e+03\n",
      "Época  2000 | Perda Total: 5.8488e+02 | Perda PDE: 1.7007e+01 | Perda IC: 3.0456e+01 | Perda BC: 5.3742e+02\n",
      "Época  3000 | Perda Total: 1.0111e+02 | Perda PDE: 2.4611e+01 | Perda IC: 1.3841e+01 | Perda BC: 6.2657e+01\n",
      "Época  4000 | Perda Total: 1.4024e+01 | Perda PDE: 2.4893e+00 | Perda IC: 2.6192e+00 | Perda BC: 8.9155e+00\n",
      "✅ Etapa concluída: Treinamento concluído\n"
     ]
    }
   ],
   "source": [
    "#(Com func. de aviso) Código para Treinamento de uma Rede Neural Profunda (PINN) para o Modelo Black-Scholes\n",
    "\n",
    "def log_step(step):\n",
    "    print(f\"✅ Etapa concluída: {step}\")\n",
    "\n",
    "try:\n",
    "    # ======================\n",
    "    # 1. Configuração\n",
    "    # ======================\n",
    "\n",
    "    # a) Parâmetros do Modelo Black-Scholes\n",
    "    r = 0.05       # Taxa livre de risco\n",
    "    sigma = 0.2    # Volatilidade\n",
    "    K = 100.0      # Preço de exercício\n",
    "    T = 1.0        # Tempo até a maturidade (anos)\n",
    "    S_min = 0      # Preço mínimo do ativo\n",
    "    S_max = 200    # Preço máximo do ativo\n",
    "\n",
    "    # b)Hiperparâmetros da PINN\n",
    "    layers = [2, 50, 50, 1]\n",
    "    lambda_pde = 1.0\n",
    "    lambda_ic = 1.0\n",
    "    lambda_bc = 1.0\n",
    "    batch_size = 100\n",
    "    epochs = 5000\n",
    "    learning_rate = 0.001\n",
    "\n",
    "    log_step(\"Configuração inicial\")\n",
    "\n",
    "    # ======================\n",
    "    # 2. Arquitetura da PINN\n",
    "    # ======================\n",
    "    class BlackScholesPINN(nn.Module):\n",
    "        def __init__(self, layers):\n",
    "            super().__init__()\n",
    "            self.linear_layers = nn.ModuleList(\n",
    "                [nn.Linear(layers[i], layers[i+1]) for i in range(len(layers)-1)]\n",
    "            )\n",
    "            self.activation = nn.Tanh()\n",
    "\n",
    "        def forward(self, t, S):\n",
    "            t_norm = t.view(-1, 1) / T\n",
    "            S_norm = S.view(-1, 1) / K\n",
    "            inputs = torch.cat([t_norm, S_norm], dim=1)\n",
    "            \n",
    "            for layer in self.linear_layers[:-1]:\n",
    "                inputs = self.activation(layer(inputs))\n",
    "            return self.linear_layers[-1](inputs)\n",
    "\n",
    "    log_step(\"Definição da arquitetura da rede\")\n",
    "\n",
    "    # ======================\n",
    "    # 3. Amostragem de Dados\n",
    "    # ======================\n",
    "    def sample_domain(batch_size):\n",
    "        t = torch.rand(batch_size, 1) * T\n",
    "        S = torch.rand(batch_size, 1) * (S_max - S_min) + S_min\n",
    "        t.requires_grad = True\n",
    "        S.requires_grad = True\n",
    "        return t, S\n",
    "\n",
    "    def sample_ic(batch_size):\n",
    "        S_ic = torch.rand(batch_size, 1) * (S_max - S_min) + S_min\n",
    "        return S_ic\n",
    "\n",
    "    def sample_bc(batch_size):\n",
    "        t_bc = torch.rand(batch_size, 1) * T\n",
    "        return t_bc\n",
    "\n",
    "    log_step(\"Amostragem de dados\")\n",
    "\n",
    "    # ======================\n",
    "    # 4. Funções de Perda\n",
    "    # ======================\n",
    "    def compute_derivatives(t, S, V_hat):\n",
    "        dV_dt = torch.autograd.grad(V_hat.sum(), t, create_graph=True)[0]\n",
    "        dV_dS = torch.autograd.grad(V_hat.sum(), S, create_graph=True)[0]\n",
    "        d2V_dS2 = torch.autograd.grad(dV_dS.sum(), S, create_graph=True)[0]\n",
    "        return dV_dt, dV_dS, d2V_dS2\n",
    "\n",
    "    def pde_loss(t, S, V_hat):\n",
    "        dV_dt, dV_dS, d2V_dS2 = compute_derivatives(t, S, V_hat)\n",
    "        residual = dV_dt + r*S*dV_dS + 0.5*(sigma**2)*(S**2)*d2V_dS2 - r*V_hat\n",
    "        return (residual**2).mean()\n",
    "\n",
    "    def ic_loss(pinn, S_ic):\n",
    "        t_ic = torch.ones_like(S_ic) * T\n",
    "        V_true = torch.max(S_ic - K, torch.zeros_like(S_ic))\n",
    "        V_pred = pinn(t_ic, S_ic)\n",
    "        return ((V_pred - V_true)**2).mean()\n",
    "\n",
    "    def bc_loss(pinn, t_bc):\n",
    "        S_bc1 = torch.zeros_like(t_bc)\n",
    "        V_bc1 = pinn(t_bc, S_bc1)\n",
    "        loss1 = (V_bc1**2).mean()\n",
    "\n",
    "        S_bc2 = torch.ones_like(t_bc) * S_max\n",
    "        V_true_bc2 = S_bc2 - K * torch.exp(-r * (T - t_bc))\n",
    "        V_bc2 = pinn(t_bc, S_bc2)\n",
    "        loss2 = ((V_bc2 - V_true_bc2)**2).mean()\n",
    "\n",
    "        return loss1 + loss2\n",
    "\n",
    "    log_step(\"Definição das funções de perda\")\n",
    "\n",
    "    # ======================\n",
    "    # 5. Loop de Treinamento\n",
    "    # ======================\n",
    "    pinn = BlackScholesPINN(layers)\n",
    "    optimizer = torch.optim.Adam(pinn.parameters(), lr=learning_rate)\n",
    "\n",
    "    loss_history = []\n",
    "\n",
    "    log_step(\"Inicialização da rede e otimizador\")\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        try:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            t_pde, S_pde = sample_domain(batch_size)\n",
    "            V_hat = pinn(t_pde, S_pde)\n",
    "\n",
    "            l_pde = pde_loss(t_pde, S_pde, V_hat)\n",
    "            l_ic = ic_loss(pinn, sample_ic(batch_size))\n",
    "            l_bc = bc_loss(pinn, sample_bc(batch_size))\n",
    "\n",
    "            loss = lambda_pde*l_pde + lambda_ic*l_ic + lambda_bc*l_bc\n",
    "            loss_history.append(loss.item())\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if epoch % 1000 == 0:\n",
    "                print(f\"Época {epoch:5d} | Perda Total: {loss.item():.4e} | \"\n",
    "                      f\"Perda PDE: {l_pde.item():.4e} | Perda IC: {l_ic.item():.4e} | \"\n",
    "                      f\"Perda BC: {l_bc.item():.4e}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Erro na época {epoch}: {e}\")\n",
    "            break  # Encerra o loop caso ocorra um erro\n",
    "\n",
    "    log_step(\"Treinamento concluído\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"❌ Ocorreu um erro durante a execução: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b396d000",
   "metadata": {},
   "source": [
    "### . Detalhes do Código"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f6cded2",
   "metadata": {},
   "source": [
    "#### 1. Configurações Iniciais\n",
    "\n",
    "1.a. Parâmetros do Modelo Black-Scholes:\n",
    "\n",
    "````\n",
    "r = 0.05       # Taxa livre de risco\n",
    "sigma = 0.2    # Volatilidade\n",
    "K = 100.0      # Preço de exercício (strike)\n",
    "T = 1.0        # Tempo até a maturidade (em anos)\n",
    "S_min = 0      # Preço mínimo do ativo\n",
    "S_max = 200    # Preço máximo do ativo\n",
    "````\n",
    "**Váriaveis representadas no código:**\n",
    "\n",
    "- **r = Taxa Livre de Risco:**\n",
    "    Representa a taxa livre de risco, ou Taxa de retorno sem risco, (r na PDE). Usada para descontar o valor futuro da opção( usada para descontar fluxos futuros ). Use a taxa anualizada de títulos públicos de prazo semelhante ao vencimento da opção, por exemplo, se a opção vence em 1 ano, pegue a taxa SELIC ou treasury bond de 1 ano. As melhores faixas são o r variando entre 1% a 15% a.a., dependendo do país e cenário econômico. Para Ajuste, pode ser atualizada periodicamente para refletir a taxa de mercado mais recente.Pode ser fixada ou modelada estocasticamente em casos mais avançados.\n",
    "    Modelagem Estocástica da Taxa Livre de Risco\n",
    "\n",
    "     Como seria uma modelagem estocástica da taxa definida?\n",
    "     A **taxa livre de risco** \\( r \\) normalmente é considerada constante para simplificar modelos de precificação. No entanto, em mercados mais complexos ou de longo prazo, essa taxa pode ser modelada como um **processo    estocástico**.\n",
    "    Modelo de Vasicek:\n",
    "        Um modelo amplamente utilizado para taxas de juros é o **Modelo de Vasicek**, que segue a equação diferencial   estocástica:\n",
    "\n",
    "    $$\n",
    "    d r_t = a (b - r_t) dt + \\sigma_r dW_t\n",
    "    $$\n",
    "\n",
    "    Onde:\n",
    "    - \\( a \\) → **Velocidade de reversão à média**: determina o quão rápido \\( r_t \\) converge para a taxa de longo     prazo.\n",
    "    - \\( b \\) → **Taxa de longo prazo (média)**: nível médio ao qual a taxa tende a se estabilizar.\n",
    "    - \\( sigma_r \\) → **Volatilidade da taxa**: grau de incerteza na variação da taxa de juros.\n",
    "    - \\( dW_t \\) → **Movimento Browniano**: captura as variações aleatórias ao longo do tempo.\n",
    "\n",
    "    Impacto da Modelagem Estocástica\n",
    "    Ao invés de considerar \\( r \\) como constante, simulamos **trajetórias possíveis** para a taxa de juros ao longo    do tempo. Isso permite:\n",
    "    - Capturar cenários onde a taxa **sobe** ou **abaixa** dinamicamente.\n",
    "    - Melhorar a modelagem de **opções de longo prazo** e **títulos de renda fixa**, tornando as simulações mais    realistas.\n",
    "\n",
    "---\n",
    "- sigma = Volatilidade do Ativo Subjacente:\n",
    "    Volatilidade do ativo subjacente (σ na PDE). Controla a difusão do preço, ou seja, mede o desvio padrão das variações do ativo. Uma boa prática é usar a volatilidade histórica ou a volatilidade implícita (se disponível no mercado de opções). Faixa comum é para Ações estáveis: 10% a 30% a.a. e Ações voláteis ou criptomoedas: 50% a 150% a.a. Para um ajuste, podemos calcular com séries históricas de preços:\n",
    "````\n",
    "volatilidade = np.std(retornos) * np.sqrt(252)\n",
    "````\n",
    "Ou use a volatilidade implícita das opções negociadas no mercado (melhor prática para precificação realista).\n",
    "\n",
    "Como seria usar a volatilidade implícita das opções negociadas no mercado?\n",
    "A volatilidade implícita é aquela que, se colocada no modelo (por exemplo, no Black-Scholes), faz com que o preço teórico da opção bata com o preço negociado no mercado. Para obter, devemos coletar os preços de opções no mercado para diferentes strikes e vencimentos. Além disso devemos usar um método de busca numérica (ex: Newton-Raphson) para encontrar a σ que iguala o preço do modelo ao preço de mercado.\n",
    "Exemplo de conceito (simplificado):\n",
    "\n",
    "````\n",
    "def erro_volatilidade(sigma):\n",
    "    preco_modelo = black_scholes(S, K, T, r, sigma)\n",
    "    return preco_modelo - preco_mercado\n",
    "\n",
    "# encontrar a sigma que zera o erro\n",
    "sigma_implicita = optimize.brentq(erro_volatilidade, 0.01, 2)\n",
    "````\n",
    "Esse tipo de operação tem como impacto refletir as expectativas de mercado sobre a volatilidade futura e uma forma mais precisa para precificar opções reais do que a volatilidade histórica.\n",
    "\n",
    "---\n",
    "\n",
    "- K = Preço de Exercício (Strike)\n",
    "    Preço de exercício (strike) da opção que representa o valor de compra/venda acordado na opção. Aparece na condição inicial (payoff final). Depende do produto financeiro. Pode ser ATM (At-the-money), ITM (In-the-money) ou OTM (Out-the-money).\n",
    "    - O que é a condição inicial (payoff final):\n",
    "        Na equação de derivadas parciais (PDE) da precificação de opções, a condição inicial é o valor da opção no momento do vencimento (t = T), ou seja, o payoff.\n",
    "        - Função de Payoff para Opções\n",
    "            Opção de Compra (*Call*) - A função de payoff para uma opção de compra (*call*) é definida como:\n",
    "            $$\n",
    "            V(S, T) = \\max(S_T - K, 0)\n",
    "            $$\n",
    "            Onde:\n",
    "                - \\( S_T \\) → Preço do ativo no tempo de vencimento \\( T \\).\n",
    "                - \\( K \\) → Preço de exercício da opção.\n",
    "                - A função **máximo** garante que o payoff seja **zero** caso \\( S_T < K \\), pois o titular da opção não a exerceria.\n",
    "            \n",
    "            Opção de Venda (*Put*) - A função de payoff para uma opção de venda (*put*) é definida como:\n",
    "\n",
    "            $$\n",
    "            V(S, T) = \\max(K - S_T, 0)\n",
    "            $$\n",
    "\n",
    "            Onde:\n",
    "                - \\( S_T \\) → Preço do ativo no tempo de vencimento \\( T \\).\n",
    "                - \\( K \\) → Preço de exercício da opção.\n",
    "                - A função **máximo** garante que o payoff seja **zero** caso \\( S_T > K \\), pois o titular da opção não a          exerceria.\n",
    "\n",
    "    Este modelo representa a base para a precificação de opções e pode ser utilizado em diversos métodos,incluindo simulações estocásticas e equações diferenciais parciais.\n",
    "\n",
    "- O que é ATM, ITM e OTM?\n",
    "    - ATM (At-the-money):\n",
    "     - Quando o preço do ativo ≈ strike.\n",
    "    - ITM (In-the-money):\n",
    "        - Para call: preço do ativo > strike\n",
    "        - Para put: preço do ativo < strike\n",
    "    - OTM (Out-the-money):\n",
    "        - Para call: preço do ativo < strike\n",
    "        - Para put: preço do ativo > strike\n",
    "\n",
    "    Em simulações, use múltiplos valores de K para construir a curva de valor da opção. Para isso, você pode precificar a opção para uma série de valores de strike e montar uma curva V(K) com o seguinte exemplo:\n",
    "    ````\n",
    "    Ks = np.linspace(80, 120, 20)\n",
    "    valores_opcao = [black_scholes(S, K, T, r, sigma) for K in Ks]\n",
    "\n",
    "    plt.plot(Ks, valores_opcao)\n",
    "    plt.xlabel(\"Preço de Exercício (K)\")\n",
    "    plt.ylabel(\"Valor da Opção\")\n",
    "    plt.title(\"Curva Valor da Opção x Strike\")\n",
    "    plt.show()\n",
    "    ````\n",
    "    As vantagens de se utilizar isso é que permite uma analise de opções sob diferentes nivéis de strike usando para contruir superfícies de volatilidade e estudar smiles ou skews\n",
    "    A faixa comum se da nas casas próximas ao preço atual do ativo, ou a ±10%, ±20%, etc., para cenários de stress. Defina de acordo com o contrato ou com a faixa que deseja testar.\n",
    "\n",
    "---\n",
    "\n",
    "- T = Tempo até o Vencimento\n",
    "    Tempo total até a expiração da opção. Define o domínio temporal t∈[0,T]. É conveniente que se expresse em anos (ex: 6 meses = 0.5). Prefira ajustes dinâmicos se for atualizar o valor da opção ao longo do tempo. Comum operar na faixa de 1 dia (≈ 0.00396 anos) até 5 anos (5.0). O ajuste é feito pelo contrato da opção e em modelos dinâmicos, diminua t continuamente conforme o tempo passa, uma vez que modelos com atualização dinâmica são mais realistas para opções com vencimento curto e o valor da opção decai com o tempo (efeito theta)\n",
    "\n",
    "    Ex: hoje T = 1 ano\n",
    "\n",
    "    Amanhã T = 1 - (1/252)\n",
    "\n",
    "    Em simulação contínua:\n",
    "\n",
    "    𝑡 → 𝑇 − passo de tempo acumulado\n",
    "\n",
    "---\n",
    "\n",
    "- S_min e S_max = — Limites do Domínio Espacial\n",
    "    Intervalo de preços considerado na simulação. Importante para as condições de contorno. S_min: geralmente 0 e S_max: no mínimo 2x o preço atual, mas prefira 3x ou 4x para evitar problemas de contorno. Exemplo:\n",
    "    - Se preço atual = 100:\n",
    "        - S_min = 0\n",
    "        - S_max = 200 a 400\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6472e7bd",
   "metadata": {},
   "source": [
    "1.b. Hiperparâmetros da PINN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5caf9628",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "feb61c13",
   "metadata": {},
   "source": [
    "#### 2. Definição da Arquitetura da PINN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa34e703",
   "metadata": {},
   "source": [
    "#### 3. Funções de Amostragem de Dados"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca6f33e",
   "metadata": {},
   "source": [
    "#### 4. Funções de Perda"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5af27399",
   "metadata": {},
   "source": [
    "#### 5. Loop de Treinamento"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25cb8524",
   "metadata": {},
   "source": [
    "#### Próximos Passos\n",
    "\n",
    "- Visualização: Plotar a solução da PINN vs. solução analítica de Black-Scholes.\n",
    "- Extensões: Adicionar dados de mercado reais (loss_data) para calibração."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15eae855",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemplo de visualização (após o treinamento)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "S_test = torch.linspace(S_min, S_max, 100).reshape(-1, 1)\n",
    "t_test = torch.zeros_like(S_test)  # t=0 (hoje)\n",
    "V_pred = pinn(t_test, S_test).detach().numpy()\n",
    "\n",
    "plt.plot(S_test, V_pred, label=\"PINN Solution\")\n",
    "plt.xlabel(\"Preço do Ativo (S)\"); plt.ylabel(\"Preço da Opção (V)\")\n",
    "plt.legend(); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0509ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================\n",
    "# 6. Visualization\n",
    "# ======================\n",
    "# Plot training loss\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.semilogy(loss_history)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss (log scale)\")\n",
    "plt.title(\"Training Convergence\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Plot option price surface\n",
    "with torch.no_grad():\n",
    "    \n",
    "    # Preço da opção (V) em função do preço do ativo (S) e do tempo (t)\n",
    "    S_test = torch.linspace(S_min, S_max, 100).reshape(-1, 1)\n",
    "    t_test = torch.zeros_like(S_test)  # t=0 (hoje)\n",
    "    V_pred = pinn(t_test, S_test).detach().numpy()\n",
    "    \n",
    "    plt.plot(S_test, V_pred, label=\"PINN Solution\")\n",
    "    plt.xlabel(\"Preço do Ativo (S)\"); plt.ylabel(\"Preço da Opção (V)\")\n",
    "    plt.legend(); plt.show()    \n",
    "\n",
    "    #Training Convergence\n",
    "    S_test = torch.linspace(S_min, S_max, 100).reshape(-1, 1)\n",
    "    t_test = torch.linspace(0, T, 50).reshape(-1, 1)\n",
    "    \n",
    "\n",
    "    \n",
    "    # Create grid for 3D plot\n",
    "    S_grid, t_grid = torch.meshgrid(S_test.squeeze(), t_test.squeeze())\n",
    "    V_grid = pinn(t_grid.reshape(-1, 1), S_grid.reshape(-1, 1))\n",
    "    V_grid = V_grid.reshape(S_grid.shape).numpy()\n",
    "    \n",
    "    fig = plt.figure(figsize=(12, 6))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    ax.plot_surface(S_grid.numpy(), t_grid.numpy(), V_grid, cmap='viridis')\n",
    "    ax.set_xlabel('Asset Price (S)')\n",
    "    ax.set_ylabel('Time to Maturity (t)')\n",
    "    ax.set_zlabel('Option Price (V)')\n",
    "    ax.set_title('PINN Solution to Black-Scholes')\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Ambiente",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
