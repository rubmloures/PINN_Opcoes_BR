{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "935f42eb",
   "metadata": {},
   "source": [
    "Sum√°rio: \n",
    "\n",
    "1. Physics-Informed Neural Networks (PINNs) com PyTorch\n",
    "\n",
    "1. **Introdu√ß√£o √†s PINNs**\n",
    "    - O que s√£o PINNs?\n",
    "    - Por que combinar f√≠sica com redes neurais?\n",
    "    - Aplica√ß√µes em finan√ßas e outras √°reas.\n",
    "\n",
    "2. **Fundamentos Matem√°ticos das PINNs**\n",
    "    - Formula√ß√£o de equa√ß√µes diferenciais parciais (PDEs) em PINNs.\n",
    "    - Fun√ß√£o de perda combinando dados e f√≠sica.\n",
    "    - Exemplo simples: Equa√ß√£o do Calor ou Black-Scholes.\n",
    "\n",
    "3. **Implementa√ß√£o B√°sica com PyTorch**\n",
    "    - Arquitetura da rede neural.\n",
    "    - Defini√ß√£o da fun√ß√£o de perda (dados + PDE).\n",
    "    - Treinamento e desafios num√©ricos.\n",
    "\n",
    "4. **Aplica√ß√£o em Finan√ßas: Modelo Black-Scholes**\n",
    "    - Revis√£o da equa√ß√£o Black-Scholes.\n",
    "    - Adapta√ß√£o para uma PINN.\n",
    "    - Compara√ß√£o com solu√ß√µes anal√≠ticas/n√∫mericas tradicionais.\n",
    "\n",
    "5. **T√≥picos Avan√ßados e Otimiza√ß√µes**\n",
    "    - Tratamento de condi√ß√µes iniciais/de contorno.\n",
    "    - Balanceamento de termos na fun√ß√£o de perda.\n",
    "    - Acelera√ß√£o com GPUs e t√©cnicas de treinamento.\n",
    "\n",
    "6. **Estudo de Caso Pr√°tico**\n",
    "    - Implementa√ß√£o completa de uma PINN para op√ß√µes europeias.\n",
    "    - Visualiza√ß√£o de resultados.\n",
    "\n",
    "7. **Limita√ß√µes e Extens√µes**\n",
    "    - Quando PINNs falham?\n",
    "    - Alternativas h√≠bridas (ex: PINNs + SDEs)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb0e2d0",
   "metadata": {},
   "source": [
    "## 1. Physics-Informed Neural Networks (PINNs) com PyTorch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f9be1e",
   "metadata": {},
   "source": [
    "### 1.1 **Introdu√ß√£o √†s PINNs**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa92f78f",
   "metadata": {},
   "source": [
    "1.1 **Introdu√ß√£o √†s PINNs**\n",
    "- O que s√£o PINNs?\n",
    "- Por que combinar f√≠sica com redes neurais?\n",
    "- Aplica√ß√µes em finan√ßas e outras √°reas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f385a3d5",
   "metadata": {},
   "source": [
    "O que s√£o PINNs?\n",
    "PINNs s√£o redes neurais treinadas para resolver equa√ß√µes diferenciais (PDEs/ODEs) incorporando diretamente as leis f√≠sicas (ou financeiras) na fun√ß√£o de perda. Em vez de depender apenas de dados, elas usam a estrutura conhecida da equa√ß√£o subjacente (ex: Black-Scholes) como regulariza√ß√£o.\n",
    "\n",
    "Por que combinar f√≠sica com redes neurais?\n",
    "- Dados escassos: Em problemas financeiros ou f√≠sicos, dados reais podem ser limitados ou ruidosos. A f√≠sica atua como um \"guia\" para generaliza√ß√£o.\n",
    "- Interpretabilidade: A solu√ß√£o respeita leis conhecidas, mesmo em regi√µes sem dados.\n",
    "- Flexibilidade: PINNs lidam com problemas n√£o-lineares e geometrias complexas sem malhas num√©ricas.\n",
    "\n",
    "Aplica√ß√µes em finan√ßas:\n",
    "- Precifica√ß√£o de op√ß√µes (Black-Scholes).\n",
    "- Calibra√ß√£o de modelos estoc√°sticos.\n",
    "- Simula√ß√£o de cen√°rios de mercado com restri√ß√µes te√≥ricas.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "197434ce",
   "metadata": {},
   "source": [
    "### 2. **Fundamentos Matem√°ticos das PINNs**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f08a066",
   "metadata": {},
   "source": [
    "**Fundamentos Matem√°ticos das PINNs**\n",
    "    - Formula√ß√£o de equa√ß√µes diferenciais parciais (PDEs) em PINNs.\n",
    "    - Fun√ß√£o de perda combinando dados e f√≠sica.\n",
    "    - Exemplo simples: Equa√ß√£o do Calor ou Black-Scholes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "954aee46",
   "metadata": {},
   "source": [
    "As PINNs resolvem problemas baseados em equa√ß√µes diferenciais (PDEs/ODEs) combinando duas fontes de informa√ß√£o:  \n",
    "1. **Dados observados** (ex: pre√ßos de op√ß√µes no mercado).  \n",
    "2. **Leis f√≠sicas/financeiras** (ex: equa√ß√£o Black-Scholes).  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba0ef56",
   "metadata": {},
   "source": [
    "#### **2.1 Formula√ß√£o Geral de uma PINN**  \n",
    "Considere uma PDE gen√©rica:  \n",
    "$$\n",
    "\\mathcal{F}[u(t, x)] = 0, \\quad t \\in [0, T], x \\in \\Omega  \n",
    "$$  \n",
    "com:  \n",
    "- **Condi√ß√£o inicial (IC):** \\( u(0, x) = u_0(x) \\).  \n",
    "- **Condi√ß√µes de contorno (BC):** \\( u(t, x) = g(t, x) \\) em \\( \\partial\\Omega \\).  \n",
    "\n",
    "**Exemplo (Equa√ß√£o Black-Scholes):**  \n",
    "$$\n",
    "\\frac{\\partial V}{\\partial t} + rS\\frac{\\partial V}{\\partial S} + \\frac{1}{2}\\sigma^2 S^2 \\frac{\\partial^2 V}{\\partial S^2} - rV = 0  \n",
    "$$  \n",
    "Onde \\( V(t, S) \\) √© o pre√ßo da op√ß√£o, \\( S \\) √© o ativo subjacente, \\( r \\) √© a taxa livre de risco, e \\( \\sigma \\) √© a volatilidade.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "883fddf0",
   "metadata": {},
   "source": [
    "A ordem l√≥gica do c√≥gido se da pela seguinte forma:\n",
    "\n",
    "C√≥digo Completo da PINN para Black-Scholes\n",
    "1. Importa√ß√µes e Configura√ß√µes Iniciais\n",
    "2. Defini√ß√£o da Arquitetura da PINN\n",
    "3. Fun√ß√µes de Amostragem de Dados\n",
    "4. Fun√ß√µes de Perda\n",
    "5. Loop de Treinamento\n",
    "\n",
    "\n",
    "Ordem L√≥gica de Execu√ß√£o\n",
    "- Configura√ß√£o: Define par√¢metros do modelo e hiperpar√¢metros da rede.\n",
    "- \n",
    "- Arquitetura: Cria a PINN com camadas lineares e ativa√ß√£o Tanh.\n",
    "- \n",
    "- Amostragem: Gera pontos para o dom√≠nio, condi√ß√£o inicial e contorno.\n",
    "- \n",
    "- Perdas: Calcula os termos da PDE, condi√ß√£o inicial e contorno.\n",
    "- \n",
    "- Treinamento: Minimiza a perda total via backpropagation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7afcbd12",
   "metadata": {},
   "source": [
    "# A Equa√ß√£o de Black-Scholes\n",
    "\n",
    "A equa√ß√£o de Black-Scholes √© uma PDE (Equa√ß√£o Diferencial Parcial) que descreve a evolu√ß√£o do pre√ßo de uma op√ß√£o europeia sob certas premissas, como:\n",
    "- Mercado eficiente;\n",
    "- Sem pagamento de dividendos;\n",
    "- Volatilidade constante.\n",
    "\n",
    "Para uma **op√ß√£o de compra (call)**, sua f√≥rmula anal√≠tica √©:\n",
    "\n",
    "$$\n",
    "C(S_t, t) = S_t N(d_1) - K e^{-r(T-t)} N(d_2)\n",
    "$$\n",
    "\n",
    "Onde:\n",
    "- \\( C(S_t, t) \\) √© o pre√ßo da op√ß√£o no tempo \\( t \\), dado o pre√ßo do ativo \\( S_t \\).\n",
    "- \\( N(\\cdot) \\) √© a fun√ß√£o cumulativa da distribui√ß√£o normal padr√£o.\n",
    "- \\( d_1 \\) e \\( d_2 \\) s√£o termos que dependem de \\( S_t \\), \\( K \\), \\( r \\), \\( \\sigma \\) e \\( T - t \\).\n",
    "\n",
    "### Forma Diferencial (PDE) da Equa√ß√£o de Black-Scholes\n",
    "\n",
    "A forma diferencial da equa√ß√£o, que √© a base para a PINN, √©:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial V}{\\partial t} + r S \\frac{\\partial V}{\\partial S} + \\frac{1}{2} \\sigma^2 S^2 \\frac{\\partial^2 V}{\\partial S^2} - rV = 0\n",
    "$$\n",
    "\n",
    "Essa equa√ß√£o √© a base para diversas abordagens num√©ricas e de aprendizado de m√°quina, como redes neurais f√≠sicas (*Physics-Informed Neural Networks* - PINNs), que buscam resolver a PDE de forma eficiente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca77e79c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#C√≥digo para Treinamento de uma Rede Neural Profunda (PINN) para o Modelo Black-Scholes\n",
    "#1. Importa√ß√µes e Configura√ß√µes Iniciais\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "# Par√¢metros do modelo Black-Scholes\n",
    "r = 0.05      # Taxa livre de risco\n",
    "sigma = 0.2   # Volatilidade\n",
    "K = 100.0     # Pre√ßo de exerc√≠cio (strike)\n",
    "T = 1.0       # Tempo at√© a expira√ß√£o (em anos)\n",
    "S_min = 0     # Pre√ßo m√≠nimo do ativo\n",
    "S_max = 200   # Pre√ßo m√°ximo do ativo\n",
    "\n",
    "# Hiperpar√¢metros da PINN\n",
    "layers = [2, 50, 50, 1]  # Arquitetura da rede: [input_dim, hidden_dim, ..., output_dim]\n",
    "lambda_pde = 1.0         # Peso da perda da PDE\n",
    "lambda_ic = 1.0          # Peso da condi√ß√£o inicial\n",
    "lambda_bc = 1.0          # Peso das condi√ß√µes de contorno\n",
    "batch_size = 100         # N√∫mero de pontos por batch\n",
    "epochs = 5000            # N√∫mero de √©pocas\n",
    "\n",
    "#2. Defini√ß√£o da Arquitetura da PINN\n",
    "class PINN(nn.Module):\n",
    "    def __init__(self, layers):\n",
    "        super().__init__()\n",
    "        self.linear_layers = nn.ModuleList(\n",
    "            [nn.Linear(layers[i], layers[i+1]) for i in range(len(layers)-1)]\n",
    "        )\n",
    "        self.activation = nn.Tanh()  # Ativa√ß√£o suave para gradientes est√°veis\n",
    "\n",
    "    def forward(self, t, S):\n",
    "        inputs = torch.cat([t, S], dim=1)  # Concatena tempo (t) e pre√ßo (S)\n",
    "        for layer in self.linear_layers[:-1]:\n",
    "            inputs = self.activation(layer(inputs))\n",
    "        output = self.linear_layers[-1](inputs)  # Sem ativa√ß√£o na √∫ltima camada\n",
    "        return output\n",
    "    \n",
    "#3. Fun√ß√µes de Amostragem de Dados\n",
    "def sample_domain(batch_size):\n",
    "    \"\"\"Gera pontos aleat√≥rios no dom√≠nio [0,T] x [S_min, S_max].\"\"\"\n",
    "    t = torch.rand(batch_size, 1) * T\n",
    "    S = torch.rand(batch_size, 1) * (S_max - S_min) + S_min\n",
    "    return t, S\n",
    "\n",
    "def sample_ic(batch_size):\n",
    "    \"\"\"Gera pontos para a condi√ß√£o inicial (payoff em t=T).\"\"\"\n",
    "    S_ic = torch.rand(batch_size, 1) * (S_max - S_min) + S_min\n",
    "    return S_ic\n",
    "\n",
    "def sample_bc_times(batch_size):\n",
    "    \"\"\"Gera tempos para as condi√ß√µes de contorno.\"\"\"\n",
    "    return torch.rand(batch_size, 1) * T\n",
    "\n",
    "#4. Fun√ß√µes de Perda\n",
    "def loss_pde(t, S, V_hat):\n",
    "    \"\"\"Calcula o residual da PDE Black-Scholes.\"\"\"\n",
    "    t.requires_grad_(True)\n",
    "    S.requires_grad_(True)\n",
    "    \n",
    "    # Derivadas parciais via autograd\n",
    "    dV_dt = torch.autograd.grad(V_hat.sum(), t, create_graph=True)[0]\n",
    "    dV_dS = torch.autograd.grad(V_hat.sum(), S, create_graph=True)[0]\n",
    "    d2V_dS2 = torch.autograd.grad(dV_dS.sum(), S, create_graph=True)[0]\n",
    "    \n",
    "    # Residual da PDE\n",
    "    residual = dV_dt + r * S * dV_dS + 0.5 * (sigma ** 2) * (S ** 2) * d2V_dS2 - r * V_hat\n",
    "    return (residual ** 2).mean()\n",
    "\n",
    "def loss_ic(S_ic):\n",
    "    \"\"\"Perda da condi√ß√£o inicial (payoff final).\"\"\"\n",
    "    t_ic = torch.ones_like(S_ic) * T  # Tempo final (t=T)\n",
    "    V_true_ic = torch.max(S_ic - K, torch.zeros_like(S_ic))  # Payoff: max(S-K, 0)\n",
    "    V_pred_ic = pinn(t_ic, S_ic)\n",
    "    return ((V_pred_ic - V_true_ic) ** 2).mean()\n",
    "\n",
    "def loss_bc(t_bc):\n",
    "    \"\"\"Perda das condi√ß√µes de contorno.\"\"\"\n",
    "    # BC1: V(t, S=0) = 0\n",
    "    S_bc_low = torch.zeros_like(t_bc)\n",
    "    V_bc_low = pinn(t_bc, S_bc_low)\n",
    "    loss_bc1 = (V_bc_low ** 2).mean()\n",
    "    \n",
    "    # BC2: V(t, S -> ‚àû) ‚âà S - K*e^{-r(T-t)}\n",
    "    S_bc_high = torch.ones_like(t_bc) * S_max\n",
    "    V_bc_high = pinn(t_bc, S_bc_high)\n",
    "    V_true_bc_high = S_bc_high - K * torch.exp(-r * (T - t_bc))\n",
    "    loss_bc2 = ((V_bc_high - V_true_bc_high) ** 2).mean()\n",
    "    \n",
    "    return loss_bc1 + loss_bc2\n",
    "\n",
    "#5. Loop de Treinamento\n",
    "# Inicializa a rede e o otimizador\n",
    "pinn = PINN(layers)\n",
    "optimizer = torch.optim.Adam(pinn.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Amostra pontos do dom√≠nio e calcula as perdas\n",
    "    t_pde, S_pde = sample_domain(batch_size)\n",
    "    V_hat = pinn(t_pde, S_pde)\n",
    "    \n",
    "    l_pde = loss_pde(t_pde, S_pde, V_hat)\n",
    "    l_ic = loss_ic(sample_ic(batch_size))\n",
    "    l_bc = loss_bc(sample_bc_times(batch_size))\n",
    "    \n",
    "    # Perda total ponderada\n",
    "    loss = lambda_pde * l_pde + lambda_ic * l_ic + lambda_bc * l_bc\n",
    "    \n",
    "    # Backpropagation\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if epoch % 1000 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss.item():.4f}, PDE Loss: {l_pde.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0a31dbd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.10.1-cp313-cp313-win_amd64.whl.metadata (11 kB)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib)\n",
      "  Downloading contourpy-1.3.1-cp313-cp313-win_amd64.whl.metadata (5.4 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib)\n",
      "  Downloading cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib)\n",
      "  Downloading fonttools-4.57.0-cp313-cp313-win_amd64.whl.metadata (104 kB)\n",
      "Collecting kiwisolver>=1.3.1 (from matplotlib)\n",
      "  Downloading kiwisolver-1.4.8-cp313-cp313-win_amd64.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: numpy>=1.23 in d:\\programas\\python e visual basics\\anaconda.python\\envs\\ambiente\\lib\\site-packages (from matplotlib) (2.2.4)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\programas\\python e visual basics\\anaconda.python\\envs\\ambiente\\lib\\site-packages (from matplotlib) (24.2)\n",
      "Collecting pillow>=8 (from matplotlib)\n",
      "  Downloading pillow-11.2.1-cp313-cp313-win_amd64.whl.metadata (9.1 kB)\n",
      "Collecting pyparsing>=2.3.1 (from matplotlib)\n",
      "  Downloading pyparsing-3.2.3-py3-none-any.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in d:\\programas\\python e visual basics\\anaconda.python\\envs\\ambiente\\lib\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in d:\\programas\\python e visual basics\\anaconda.python\\envs\\ambiente\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Downloading matplotlib-3.10.1-cp313-cp313-win_amd64.whl (8.1 MB)\n",
      "   ---------------------------------------- 0.0/8.1 MB ? eta -:--:--\n",
      "   ---------- ----------------------------- 2.1/8.1 MB 11.7 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 4.5/8.1 MB 11.7 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 7.1/8.1 MB 11.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 8.1/8.1 MB 11.5 MB/s eta 0:00:00\n",
      "Downloading contourpy-1.3.1-cp313-cp313-win_amd64.whl (220 kB)\n",
      "Downloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Downloading fonttools-4.57.0-cp313-cp313-win_amd64.whl (2.2 MB)\n",
      "   ---------------------------------------- 0.0/2.2 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.2/2.2 MB 11.2 MB/s eta 0:00:00\n",
      "Downloading kiwisolver-1.4.8-cp313-cp313-win_amd64.whl (71 kB)\n",
      "Downloading pillow-11.2.1-cp313-cp313-win_amd64.whl (2.7 MB)\n",
      "   ---------------------------------------- 0.0/2.7 MB ? eta -:--:--\n",
      "   ----------------------------------- ---- 2.4/2.7 MB 11.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.7/2.7 MB 11.1 MB/s eta 0:00:00\n",
      "Downloading pyparsing-3.2.3-py3-none-any.whl (111 kB)\n",
      "Installing collected packages: pyparsing, pillow, kiwisolver, fonttools, cycler, contourpy, matplotlib\n",
      "Successfully installed contourpy-1.3.1 cycler-0.12.1 fonttools-4.57.0 kiwisolver-1.4.8 matplotlib-3.10.1 pillow-11.2.1 pyparsing-3.2.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d14baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af33db73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "√âpoca     0 | Perda Total: 1.2887e+04 | Perda PDE: 1.0179e-02 | Perda IC: 2.3319e+03 | Perda BC: 1.0555e+04\n",
      "√âpoca  1000 | Perda Total: 3.0909e+03 | Perda PDE: 9.0591e+00 | Perda IC: 2.8809e+02 | Perda BC: 2.7937e+03\n",
      "√âpoca  2000 | Perda Total: 5.7378e+02 | Perda PDE: 1.9597e+01 | Perda IC: 2.8109e+01 | Perda BC: 5.2608e+02\n",
      "√âpoca  3000 | Perda Total: 9.5734e+01 | Perda PDE: 8.6751e+00 | Perda IC: 8.4406e+00 | Perda BC: 7.8618e+01\n",
      "√âpoca  4000 | Perda Total: 1.6335e+01 | Perda PDE: 9.0750e-01 | Perda IC: 1.0315e+00 | Perda BC: 1.4396e+01\n"
     ]
    }
   ],
   "source": [
    "#C√≥digo para Treinamento de uma Rede Neural Profunda (PINN) para o Modelo Black-Scholes\n",
    "\n",
    "# ======================\n",
    "# 1. Configura√ß√£o\n",
    "# ======================\n",
    "r = 0.05       # Taxa livre de risco\n",
    "sigma = 0.2    # Volatilidade\n",
    "K = 100.0      # Pre√ßo de exerc√≠cio\n",
    "T = 1.0        # Tempo at√© a maturidade (anos)\n",
    "S_min = 0      # Pre√ßo m√≠nimo do ativo\n",
    "S_max = 200    # Pre√ßo m√°ximo do ativo\n",
    "\n",
    "# Hiperpar√¢metros da PINN\n",
    "layers = [2, 50, 50, 1]  # Estrutura da rede [entrada, ocultas..., sa√≠da]\n",
    "lambda_pde = 1.0         # Peso da perda da PDE\n",
    "lambda_ic = 1.0          # Peso da perda da condi√ß√£o inicial\n",
    "lambda_bc = 1.0          # Peso da perda da condi√ß√£o de contorno\n",
    "batch_size = 100         # Pontos por lote\n",
    "epochs = 5000            # N√∫mero de √©pocas de treino\n",
    "learning_rate = 0.001    # Taxa de aprendizado do otimizador\n",
    "\n",
    "# ======================\n",
    "# 2. Arquitetura da PINN\n",
    "# ======================\n",
    "class BlackScholesPINN(nn.Module):\n",
    "    def __init__(self, layers):\n",
    "        super().__init__()\n",
    "        self.linear_layers = nn.ModuleList(\n",
    "            [nn.Linear(layers[i], layers[i+1]) for i in range(len(layers)-1)]\n",
    "        )\n",
    "        self.activation = nn.Tanh()  # Fun√ß√£o de ativa√ß√£o suave para estabilidade\n",
    "\n",
    "    def forward(self, t, S):\n",
    "        t_norm = t.view(-1, 1) / T\n",
    "        S_norm = S.view(-1, 1) / K\n",
    "        inputs = torch.cat([t_norm, S_norm], dim=1)\n",
    "\n",
    "        for layer in self.linear_layers[:-1]:\n",
    "            inputs = self.activation(layer(inputs))\n",
    "        return self.linear_layers[-1](inputs)\n",
    "\n",
    "# ======================\n",
    "# 3. Amostragem de Dados\n",
    "# ======================\n",
    "def sample_domain(batch_size):\n",
    "    t = torch.rand(batch_size, 1) * T\n",
    "    S = torch.rand(batch_size, 1) * (S_max - S_min) + S_min\n",
    "    t.requires_grad = True\n",
    "    S.requires_grad = True\n",
    "    return t, S\n",
    "\n",
    "def sample_ic(batch_size):\n",
    "    S_ic = torch.rand(batch_size, 1) * (S_max - S_min) + S_min\n",
    "    return S_ic\n",
    "\n",
    "def sample_bc(batch_size):\n",
    "    t_bc = torch.rand(batch_size, 1) * T\n",
    "    return t_bc\n",
    "\n",
    "# ======================\n",
    "# 4. Fun√ß√µes de Perda\n",
    "# ======================\n",
    "def compute_derivatives(t, S, V_hat):\n",
    "    dV_dt = torch.autograd.grad(V_hat.sum(), t, create_graph=True)[0]\n",
    "    dV_dS = torch.autograd.grad(V_hat.sum(), S, create_graph=True)[0]\n",
    "    d2V_dS2 = torch.autograd.grad(dV_dS.sum(), S, create_graph=True)[0]\n",
    "    return dV_dt, dV_dS, d2V_dS2\n",
    "\n",
    "def pde_loss(t, S, V_hat):\n",
    "    dV_dt, dV_dS, d2V_dS2 = compute_derivatives(t, S, V_hat)\n",
    "    residual = dV_dt + r*S*dV_dS + 0.5*(sigma**2)*(S**2)*d2V_dS2 - r*V_hat\n",
    "    return (residual**2).mean()\n",
    "\n",
    "def ic_loss(pinn, S_ic):\n",
    "    t_ic = torch.ones_like(S_ic) * T\n",
    "    V_true = torch.max(S_ic - K, torch.zeros_like(S_ic))\n",
    "    V_pred = pinn(t_ic, S_ic)\n",
    "    return ((V_pred - V_true)**2).mean()\n",
    "\n",
    "def bc_loss(pinn, t_bc):\n",
    "    S_bc1 = torch.zeros_like(t_bc)\n",
    "    V_bc1 = pinn(t_bc, S_bc1)\n",
    "    loss1 = (V_bc1**2).mean()\n",
    "\n",
    "    S_bc2 = torch.ones_like(t_bc) * S_max\n",
    "    V_true_bc2 = S_bc2 - K * torch.exp(-r * (T - t_bc))\n",
    "    V_bc2 = pinn(t_bc, S_bc2)\n",
    "    loss2 = ((V_bc2 - V_true_bc2)**2).mean()\n",
    "\n",
    "    return loss1 + loss2\n",
    "\n",
    "# ======================\n",
    "# 5. Loop de Treinamento\n",
    "# ======================\n",
    "pinn = BlackScholesPINN(layers)\n",
    "optimizer = torch.optim.Adam(pinn.parameters(), lr=learning_rate)\n",
    "\n",
    "loss_history = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    t_pde, S_pde = sample_domain(batch_size)\n",
    "    V_hat = pinn(t_pde, S_pde)\n",
    "\n",
    "    l_pde = pde_loss(t_pde, S_pde, V_hat)\n",
    "    l_ic = ic_loss(pinn, sample_ic(batch_size))\n",
    "    l_bc = bc_loss(pinn, sample_bc(batch_size))\n",
    "\n",
    "    loss = lambda_pde*l_pde + lambda_ic*l_ic + lambda_bc*l_bc\n",
    "    loss_history.append(loss.item())\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 1000 == 0:\n",
    "        print(f\"√âpoca {epoch:5d} | Perda Total: {loss.item():.4e} | \"\n",
    "              f\"Perda PDE: {l_pde.item():.4e} | Perda IC: {l_ic.item():.4e} | \"\n",
    "              f\"Perda BC: {l_bc.item():.4e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc8ad46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Etapa conclu√≠da: Configura√ß√£o inicial\n",
      "‚úÖ Etapa conclu√≠da: Defini√ß√£o da arquitetura da rede\n",
      "‚úÖ Etapa conclu√≠da: Amostragem de dados\n",
      "‚úÖ Etapa conclu√≠da: Defini√ß√£o das fun√ß√µes de perda\n",
      "‚úÖ Etapa conclu√≠da: Inicializa√ß√£o da rede e otimizador\n",
      "√âpoca     0 | Perda Total: 1.2130e+04 | Perda PDE: 3.0908e-02 | Perda IC: 1.5536e+03 | Perda BC: 1.0576e+04\n",
      "√âpoca  1000 | Perda Total: 3.0298e+03 | Perda PDE: 6.2679e+00 | Perda IC: 2.3360e+02 | Perda BC: 2.7899e+03\n",
      "√âpoca  2000 | Perda Total: 5.8488e+02 | Perda PDE: 1.7007e+01 | Perda IC: 3.0456e+01 | Perda BC: 5.3742e+02\n",
      "√âpoca  3000 | Perda Total: 1.0111e+02 | Perda PDE: 2.4611e+01 | Perda IC: 1.3841e+01 | Perda BC: 6.2657e+01\n",
      "√âpoca  4000 | Perda Total: 1.4024e+01 | Perda PDE: 2.4893e+00 | Perda IC: 2.6192e+00 | Perda BC: 8.9155e+00\n",
      "‚úÖ Etapa conclu√≠da: Treinamento conclu√≠do\n"
     ]
    }
   ],
   "source": [
    "#(Com func. de aviso) C√≥digo para Treinamento de uma Rede Neural Profunda (PINN) para o Modelo Black-Scholes\n",
    "\n",
    "def log_step(step):\n",
    "    print(f\"‚úÖ Etapa conclu√≠da: {step}\")\n",
    "\n",
    "try:\n",
    "    # ======================\n",
    "    # 1. Configura√ß√£o\n",
    "    # ======================\n",
    "\n",
    "    # a) Par√¢metros do Modelo Black-Scholes\n",
    "    r = 0.05       # Taxa livre de risco\n",
    "    sigma = 0.2    # Volatilidade\n",
    "    K = 100.0      # Pre√ßo de exerc√≠cio\n",
    "    T = 1.0        # Tempo at√© a maturidade (anos)\n",
    "    S_min = 0      # Pre√ßo m√≠nimo do ativo\n",
    "    S_max = 200    # Pre√ßo m√°ximo do ativo\n",
    "\n",
    "    # b)Hiperpar√¢metros da PINN\n",
    "    layers = [2, 50, 50, 1]\n",
    "    lambda_pde = 1.0\n",
    "    lambda_ic = 1.0\n",
    "    lambda_bc = 1.0\n",
    "    batch_size = 100\n",
    "    epochs = 5000\n",
    "    learning_rate = 0.001\n",
    "\n",
    "    log_step(\"Configura√ß√£o inicial\")\n",
    "\n",
    "    # ======================\n",
    "    # 2. Arquitetura da PINN\n",
    "    # ======================\n",
    "    class BlackScholesPINN(nn.Module):\n",
    "        def __init__(self, layers):\n",
    "            super().__init__()\n",
    "            self.linear_layers = nn.ModuleList(\n",
    "                [nn.Linear(layers[i], layers[i+1]) for i in range(len(layers)-1)]\n",
    "            )\n",
    "            self.activation = nn.Tanh()\n",
    "\n",
    "        def forward(self, t, S):\n",
    "            t_norm = t.view(-1, 1) / T\n",
    "            S_norm = S.view(-1, 1) / K\n",
    "            inputs = torch.cat([t_norm, S_norm], dim=1)\n",
    "            \n",
    "            for layer in self.linear_layers[:-1]:\n",
    "                inputs = self.activation(layer(inputs))\n",
    "            return self.linear_layers[-1](inputs)\n",
    "\n",
    "    log_step(\"Defini√ß√£o da arquitetura da rede\")\n",
    "\n",
    "    # ======================\n",
    "    # 3. Amostragem de Dados\n",
    "    # ======================\n",
    "    def sample_domain(batch_size):\n",
    "        t = torch.rand(batch_size, 1) * T\n",
    "        S = torch.rand(batch_size, 1) * (S_max - S_min) + S_min\n",
    "        t.requires_grad = True\n",
    "        S.requires_grad = True\n",
    "        return t, S\n",
    "\n",
    "    def sample_ic(batch_size):\n",
    "        S_ic = torch.rand(batch_size, 1) * (S_max - S_min) + S_min\n",
    "        return S_ic\n",
    "\n",
    "    def sample_bc(batch_size):\n",
    "        t_bc = torch.rand(batch_size, 1) * T\n",
    "        return t_bc\n",
    "\n",
    "    log_step(\"Amostragem de dados\")\n",
    "\n",
    "    # ======================\n",
    "    # 4. Fun√ß√µes de Perda\n",
    "    # ======================\n",
    "    def compute_derivatives(t, S, V_hat):\n",
    "        dV_dt = torch.autograd.grad(V_hat.sum(), t, create_graph=True)[0]\n",
    "        dV_dS = torch.autograd.grad(V_hat.sum(), S, create_graph=True)[0]\n",
    "        d2V_dS2 = torch.autograd.grad(dV_dS.sum(), S, create_graph=True)[0]\n",
    "        return dV_dt, dV_dS, d2V_dS2\n",
    "\n",
    "    def pde_loss(t, S, V_hat):\n",
    "        dV_dt, dV_dS, d2V_dS2 = compute_derivatives(t, S, V_hat)\n",
    "        residual = dV_dt + r*S*dV_dS + 0.5*(sigma**2)*(S**2)*d2V_dS2 - r*V_hat\n",
    "        return (residual**2).mean()\n",
    "\n",
    "    def ic_loss(pinn, S_ic):\n",
    "        t_ic = torch.ones_like(S_ic) * T\n",
    "        V_true = torch.max(S_ic - K, torch.zeros_like(S_ic))\n",
    "        V_pred = pinn(t_ic, S_ic)\n",
    "        return ((V_pred - V_true)**2).mean()\n",
    "\n",
    "    def bc_loss(pinn, t_bc):\n",
    "        S_bc1 = torch.zeros_like(t_bc)\n",
    "        V_bc1 = pinn(t_bc, S_bc1)\n",
    "        loss1 = (V_bc1**2).mean()\n",
    "\n",
    "        S_bc2 = torch.ones_like(t_bc) * S_max\n",
    "        V_true_bc2 = S_bc2 - K * torch.exp(-r * (T - t_bc))\n",
    "        V_bc2 = pinn(t_bc, S_bc2)\n",
    "        loss2 = ((V_bc2 - V_true_bc2)**2).mean()\n",
    "\n",
    "        return loss1 + loss2\n",
    "\n",
    "    log_step(\"Defini√ß√£o das fun√ß√µes de perda\")\n",
    "\n",
    "    # ======================\n",
    "    # 5. Loop de Treinamento\n",
    "    # ======================\n",
    "    pinn = BlackScholesPINN(layers)\n",
    "    optimizer = torch.optim.Adam(pinn.parameters(), lr=learning_rate)\n",
    "\n",
    "    loss_history = []\n",
    "\n",
    "    log_step(\"Inicializa√ß√£o da rede e otimizador\")\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        try:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            t_pde, S_pde = sample_domain(batch_size)\n",
    "            V_hat = pinn(t_pde, S_pde)\n",
    "\n",
    "            l_pde = pde_loss(t_pde, S_pde, V_hat)\n",
    "            l_ic = ic_loss(pinn, sample_ic(batch_size))\n",
    "            l_bc = bc_loss(pinn, sample_bc(batch_size))\n",
    "\n",
    "            loss = lambda_pde*l_pde + lambda_ic*l_ic + lambda_bc*l_bc\n",
    "            loss_history.append(loss.item())\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if epoch % 1000 == 0:\n",
    "                print(f\"√âpoca {epoch:5d} | Perda Total: {loss.item():.4e} | \"\n",
    "                      f\"Perda PDE: {l_pde.item():.4e} | Perda IC: {l_ic.item():.4e} | \"\n",
    "                      f\"Perda BC: {l_bc.item():.4e}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Erro na √©poca {epoch}: {e}\")\n",
    "            break  # Encerra o loop caso ocorra um erro\n",
    "\n",
    "    log_step(\"Treinamento conclu√≠do\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Ocorreu um erro durante a execu√ß√£o: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b396d000",
   "metadata": {},
   "source": [
    "### . Detalhes do C√≥digo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f6cded2",
   "metadata": {},
   "source": [
    "#### 1. Configura√ß√µes Iniciais\n",
    "\n",
    "1.a. Par√¢metros do Modelo Black-Scholes:\n",
    "\n",
    "````\n",
    "r = 0.05       # Taxa livre de risco\n",
    "sigma = 0.2    # Volatilidade\n",
    "K = 100.0      # Pre√ßo de exerc√≠cio (strike)\n",
    "T = 1.0        # Tempo at√© a maturidade (em anos)\n",
    "S_min = 0      # Pre√ßo m√≠nimo do ativo\n",
    "S_max = 200    # Pre√ßo m√°ximo do ativo\n",
    "````\n",
    "**V√°riaveis representadas no c√≥digo:**\n",
    "\n",
    "- **r = Taxa Livre de Risco:**\n",
    "    Representa a taxa livre de risco, ou Taxa de retorno sem risco, (r na PDE). Usada para descontar o valor futuro da op√ß√£o( usada para descontar fluxos futuros ). Use a taxa anualizada de t√≠tulos p√∫blicos de prazo semelhante ao vencimento da op√ß√£o, por exemplo, se a op√ß√£o vence em 1 ano, pegue a taxa SELIC ou treasury bond de 1 ano. As melhores faixas s√£o o r variando entre 1% a 15% a.a., dependendo do pa√≠s e cen√°rio econ√¥mico. Para Ajuste, pode ser atualizada periodicamente para refletir a taxa de mercado mais recente.Pode ser fixada ou modelada estocasticamente em casos mais avan√ßados.\n",
    "    Modelagem Estoc√°stica da Taxa Livre de Risco\n",
    "\n",
    "     Como seria uma modelagem estoc√°stica da taxa definida?\n",
    "     A **taxa livre de risco** \\( r \\) normalmente √© considerada constante para simplificar modelos de precifica√ß√£o. No entanto, em mercados mais complexos ou de longo prazo, essa taxa pode ser modelada como um **processo    estoc√°stico**.\n",
    "    Modelo de Vasicek:\n",
    "        Um modelo amplamente utilizado para taxas de juros √© o **Modelo de Vasicek**, que segue a equa√ß√£o diferencial   estoc√°stica:\n",
    "\n",
    "    $$\n",
    "    d r_t = a (b - r_t) dt + \\sigma_r dW_t\n",
    "    $$\n",
    "\n",
    "    Onde:\n",
    "    - \\( a \\) ‚Üí **Velocidade de revers√£o √† m√©dia**: determina o qu√£o r√°pido \\( r_t \\) converge para a taxa de longo     prazo.\n",
    "    - \\( b \\) ‚Üí **Taxa de longo prazo (m√©dia)**: n√≠vel m√©dio ao qual a taxa tende a se estabilizar.\n",
    "    - \\( sigma_r \\) ‚Üí **Volatilidade da taxa**: grau de incerteza na varia√ß√£o da taxa de juros.\n",
    "    - \\( dW_t \\) ‚Üí **Movimento Browniano**: captura as varia√ß√µes aleat√≥rias ao longo do tempo.\n",
    "\n",
    "    Impacto da Modelagem Estoc√°stica\n",
    "    Ao inv√©s de considerar \\( r \\) como constante, simulamos **trajet√≥rias poss√≠veis** para a taxa de juros ao longo    do tempo. Isso permite:\n",
    "    - Capturar cen√°rios onde a taxa **sobe** ou **abaixa** dinamicamente.\n",
    "    - Melhorar a modelagem de **op√ß√µes de longo prazo** e **t√≠tulos de renda fixa**, tornando as simula√ß√µes mais    realistas.\n",
    "\n",
    "---\n",
    "- sigma = Volatilidade do Ativo Subjacente:\n",
    "    Volatilidade do ativo subjacente (œÉ na PDE). Controla a difus√£o do pre√ßo, ou seja, mede o desvio padr√£o das varia√ß√µes do ativo. Uma boa pr√°tica √© usar a volatilidade hist√≥rica ou a volatilidade impl√≠cita (se dispon√≠vel no mercado de op√ß√µes). Faixa comum √© para A√ß√µes est√°veis: 10% a 30% a.a. e A√ß√µes vol√°teis ou criptomoedas: 50% a 150% a.a. Para um ajuste, podemos calcular com s√©ries hist√≥ricas de pre√ßos:\n",
    "````\n",
    "volatilidade = np.std(retornos) * np.sqrt(252)\n",
    "````\n",
    "Ou use a volatilidade impl√≠cita das op√ß√µes negociadas no mercado (melhor pr√°tica para precifica√ß√£o realista).\n",
    "\n",
    "Como seria usar a volatilidade impl√≠cita das op√ß√µes negociadas no mercado?\n",
    "A volatilidade impl√≠cita √© aquela que, se colocada no modelo (por exemplo, no Black-Scholes), faz com que o pre√ßo te√≥rico da op√ß√£o bata com o pre√ßo negociado no mercado. Para obter, devemos coletar os pre√ßos de op√ß√µes no mercado para diferentes strikes e vencimentos. Al√©m disso devemos usar um m√©todo de busca num√©rica (ex: Newton-Raphson) para encontrar a œÉ que iguala o pre√ßo do modelo ao pre√ßo de mercado.\n",
    "Exemplo de conceito (simplificado):\n",
    "\n",
    "````\n",
    "def erro_volatilidade(sigma):\n",
    "    preco_modelo = black_scholes(S, K, T, r, sigma)\n",
    "    return preco_modelo - preco_mercado\n",
    "\n",
    "# encontrar a sigma que zera o erro\n",
    "sigma_implicita = optimize.brentq(erro_volatilidade, 0.01, 2)\n",
    "````\n",
    "Esse tipo de opera√ß√£o tem como impacto refletir as expectativas de mercado sobre a volatilidade futura e uma forma mais precisa para precificar op√ß√µes reais do que a volatilidade hist√≥rica.\n",
    "\n",
    "---\n",
    "\n",
    "- K = Pre√ßo de Exerc√≠cio (Strike)\n",
    "    Pre√ßo de exerc√≠cio (strike) da op√ß√£o que representa o valor de compra/venda acordado na op√ß√£o. Aparece na condi√ß√£o inicial (payoff final). Depende do produto financeiro. Pode ser ATM (At-the-money), ITM (In-the-money) ou OTM (Out-the-money).\n",
    "    - O que √© a condi√ß√£o inicial (payoff final):\n",
    "        Na equa√ß√£o de derivadas parciais (PDE) da precifica√ß√£o de op√ß√µes, a condi√ß√£o inicial √© o valor da op√ß√£o no momento do vencimento (t = T), ou seja, o payoff.\n",
    "        - Fun√ß√£o de Payoff para Op√ß√µes\n",
    "            Op√ß√£o de Compra (*Call*) - A fun√ß√£o de payoff para uma op√ß√£o de compra (*call*) √© definida como:\n",
    "            $$\n",
    "            V(S, T) = \\max(S_T - K, 0)\n",
    "            $$\n",
    "            Onde:\n",
    "                - \\( S_T \\) ‚Üí Pre√ßo do ativo no tempo de vencimento \\( T \\).\n",
    "                - \\( K \\) ‚Üí Pre√ßo de exerc√≠cio da op√ß√£o.\n",
    "                - A fun√ß√£o **m√°ximo** garante que o payoff seja **zero** caso \\( S_T < K \\), pois o titular da op√ß√£o n√£o a exerceria.\n",
    "            \n",
    "            Op√ß√£o de Venda (*Put*) - A fun√ß√£o de payoff para uma op√ß√£o de venda (*put*) √© definida como:\n",
    "\n",
    "            $$\n",
    "            V(S, T) = \\max(K - S_T, 0)\n",
    "            $$\n",
    "\n",
    "            Onde:\n",
    "                - \\( S_T \\) ‚Üí Pre√ßo do ativo no tempo de vencimento \\( T \\).\n",
    "                - \\( K \\) ‚Üí Pre√ßo de exerc√≠cio da op√ß√£o.\n",
    "                - A fun√ß√£o **m√°ximo** garante que o payoff seja **zero** caso \\( S_T > K \\), pois o titular da op√ß√£o n√£o a          exerceria.\n",
    "\n",
    "    Este modelo representa a base para a precifica√ß√£o de op√ß√µes e pode ser utilizado em diversos m√©todos,incluindo simula√ß√µes estoc√°sticas e equa√ß√µes diferenciais parciais.\n",
    "\n",
    "- O que √© ATM, ITM e OTM?\n",
    "    - ATM (At-the-money):\n",
    "     - Quando o pre√ßo do ativo ‚âà strike.\n",
    "    - ITM (In-the-money):\n",
    "        - Para call: pre√ßo do ativo > strike\n",
    "        - Para put: pre√ßo do ativo < strike\n",
    "    - OTM (Out-the-money):\n",
    "        - Para call: pre√ßo do ativo < strike\n",
    "        - Para put: pre√ßo do ativo > strike\n",
    "\n",
    "    Em simula√ß√µes, use m√∫ltiplos valores de K para construir a curva de valor da op√ß√£o. Para isso, voc√™ pode precificar a op√ß√£o para uma s√©rie de valores de strike e montar uma curva V(K) com o seguinte exemplo:\n",
    "    ````\n",
    "    Ks = np.linspace(80, 120, 20)\n",
    "    valores_opcao = [black_scholes(S, K, T, r, sigma) for K in Ks]\n",
    "\n",
    "    plt.plot(Ks, valores_opcao)\n",
    "    plt.xlabel(\"Pre√ßo de Exerc√≠cio (K)\")\n",
    "    plt.ylabel(\"Valor da Op√ß√£o\")\n",
    "    plt.title(\"Curva Valor da Op√ß√£o x Strike\")\n",
    "    plt.show()\n",
    "    ````\n",
    "    As vantagens de se utilizar isso √© que permite uma analise de op√ß√µes sob diferentes niv√©is de strike usando para contruir superf√≠cies de volatilidade e estudar smiles ou skews\n",
    "    A faixa comum se da nas casas pr√≥ximas ao pre√ßo atual do ativo, ou a ¬±10%, ¬±20%, etc., para cen√°rios de stress. Defina de acordo com o contrato ou com a faixa que deseja testar.\n",
    "\n",
    "---\n",
    "\n",
    "- T = Tempo at√© o Vencimento\n",
    "    Tempo total at√© a expira√ß√£o da op√ß√£o. Define o dom√≠nio temporal t‚àà[0,T]. √â conveniente que se expresse em anos (ex: 6 meses = 0.5). Prefira ajustes din√¢micos se for atualizar o valor da op√ß√£o ao longo do tempo. Comum operar na faixa de 1 dia (‚âà 0.00396 anos) at√© 5 anos (5.0). O ajuste √© feito pelo contrato da op√ß√£o e em modelos din√¢micos, diminua t continuamente conforme o tempo passa, uma vez que modelos com atualiza√ß√£o din√¢mica s√£o mais realistas para op√ß√µes com vencimento curto e o valor da op√ß√£o decai com o tempo (efeito theta)\n",
    "\n",
    "    Ex: hoje T = 1 ano\n",
    "\n",
    "    Amanh√£ T = 1 - (1/252)\n",
    "\n",
    "    Em simula√ß√£o cont√≠nua:\n",
    "\n",
    "    ùë° ‚Üí ùëá ‚àí passo¬†de¬†tempo¬†acumulado\n",
    "\n",
    "---\n",
    "\n",
    "- S_min e S_max = ‚Äî Limites do Dom√≠nio Espacial\n",
    "    Intervalo de pre√ßos considerado na simula√ß√£o. Importante para as condi√ß√µes de contorno. S_min: geralmente 0 e S_max: no m√≠nimo 2x o pre√ßo atual, mas prefira 3x ou 4x para evitar problemas de contorno. Exemplo:\n",
    "    - Se pre√ßo atual = 100:\n",
    "        - S_min = 0\n",
    "        - S_max = 200 a 400\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6472e7bd",
   "metadata": {},
   "source": [
    "1.b. Hiperpar√¢metros da PINN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5caf9628",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "feb61c13",
   "metadata": {},
   "source": [
    "#### 2. Defini√ß√£o da Arquitetura da PINN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa34e703",
   "metadata": {},
   "source": [
    "#### 3. Fun√ß√µes de Amostragem de Dados"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca6f33e",
   "metadata": {},
   "source": [
    "#### 4. Fun√ß√µes de Perda"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5af27399",
   "metadata": {},
   "source": [
    "#### 5. Loop de Treinamento"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25cb8524",
   "metadata": {},
   "source": [
    "#### Pr√≥ximos Passos\n",
    "\n",
    "- Visualiza√ß√£o: Plotar a solu√ß√£o da PINN vs. solu√ß√£o anal√≠tica de Black-Scholes.\n",
    "- Extens√µes: Adicionar dados de mercado reais (loss_data) para calibra√ß√£o."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15eae855",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemplo de visualiza√ß√£o (ap√≥s o treinamento)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "S_test = torch.linspace(S_min, S_max, 100).reshape(-1, 1)\n",
    "t_test = torch.zeros_like(S_test)  # t=0 (hoje)\n",
    "V_pred = pinn(t_test, S_test).detach().numpy()\n",
    "\n",
    "plt.plot(S_test, V_pred, label=\"PINN Solution\")\n",
    "plt.xlabel(\"Pre√ßo do Ativo (S)\"); plt.ylabel(\"Pre√ßo da Op√ß√£o (V)\")\n",
    "plt.legend(); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0509ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================\n",
    "# 6. Visualization\n",
    "# ======================\n",
    "# Plot training loss\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.semilogy(loss_history)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss (log scale)\")\n",
    "plt.title(\"Training Convergence\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Plot option price surface\n",
    "with torch.no_grad():\n",
    "    \n",
    "    # Pre√ßo da op√ß√£o (V) em fun√ß√£o do pre√ßo do ativo (S) e do tempo (t)\n",
    "    S_test = torch.linspace(S_min, S_max, 100).reshape(-1, 1)\n",
    "    t_test = torch.zeros_like(S_test)  # t=0 (hoje)\n",
    "    V_pred = pinn(t_test, S_test).detach().numpy()\n",
    "    \n",
    "    plt.plot(S_test, V_pred, label=\"PINN Solution\")\n",
    "    plt.xlabel(\"Pre√ßo do Ativo (S)\"); plt.ylabel(\"Pre√ßo da Op√ß√£o (V)\")\n",
    "    plt.legend(); plt.show()    \n",
    "\n",
    "    #Training Convergence\n",
    "    S_test = torch.linspace(S_min, S_max, 100).reshape(-1, 1)\n",
    "    t_test = torch.linspace(0, T, 50).reshape(-1, 1)\n",
    "    \n",
    "\n",
    "    \n",
    "    # Create grid for 3D plot\n",
    "    S_grid, t_grid = torch.meshgrid(S_test.squeeze(), t_test.squeeze())\n",
    "    V_grid = pinn(t_grid.reshape(-1, 1), S_grid.reshape(-1, 1))\n",
    "    V_grid = V_grid.reshape(S_grid.shape).numpy()\n",
    "    \n",
    "    fig = plt.figure(figsize=(12, 6))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    ax.plot_surface(S_grid.numpy(), t_grid.numpy(), V_grid, cmap='viridis')\n",
    "    ax.set_xlabel('Asset Price (S)')\n",
    "    ax.set_ylabel('Time to Maturity (t)')\n",
    "    ax.set_zlabel('Option Price (V)')\n",
    "    ax.set_title('PINN Solution to Black-Scholes')\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Ambiente",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
